<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Ziwei Wang</title>
  
  <meta name="author" content="Ziwei Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziwei Wang</name>
              </p>
              <p>Ziwei Wang is a PhD candidate at the <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://www.au.tsinghua.edu.cn/info/1078/1627.htm">Jiwen Lu</a>.
                 He received the BS degree from the Department of Physics, Tsinghua University, China, in 2018.
                 His research interests include model compression, compact representation learning and visual reasoning. He has published 7 scientific papers in the IEEE Transactions on Pattern Analysis and Machine Intelligence, CVPR and ECCV. 
                 He serves as a regular reviewer member for the IEEE Transactions on Image Processing, Pattern Recognition Letters, CVPR, NeurIPS, ICPR and ICIP.
              </p>
              <p style="text-align:center">
                <a href="wang-zw18@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cMTW09EAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                 <a href="https://github.com/ZiweiWangTHU">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/icon.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/icon.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
               <li style="margin: 5px;" >
                <b>2020-07:</b> One paper on active hashing is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-04:</b> One paper on network quantization is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
              </li>
              <li style="margin: 5px;">
                <b>2020-02:</b> One paper on efficient detection is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-02:</b> One paper on network quantization is accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
              </p>
            </td>
          </tr>
        </tbody></table>
       

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DH-APS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Hashing with Active Pairwise Supervision</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Quan Zheng, Jiwen Lu, Jie Zhou
              <br>
              <em>16th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="data/3281.pdf">[PDF]</a>
              <a href="data/3281-slides.pdf">[Slides]</a>
              <a href="data/3281-short video.pdf">[Video]</a>
              <br>
              <p></p>
              <p>We propose a Deep Hashing method with Active Pairwise Supervision (DH-APS) to learn effective binary codes for image search with limited annotation budget.</p>
            </td>
          </tr>
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/HCI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-Wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>,  Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 17.86)</em>, 2020, accepted.
              <br>
              <a href="data/channel-wise interactions.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a>
              <br>
              <p> We present a hierarchical channel-wise interaction based binary convolutional neural networks (HCI-BCNN) method to minimize the quantiztaion error for activations via hierarchical reinforcement learning. </p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/BiDet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BiDet: An Efficient Binarized Object Detector</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Ziyi Wu, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="data/05105.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              <p>We propose an efficient binarized object detector that fully utilizes the representational capacity of the binary neural networks by redundancy removal</strong>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Chenxin Tao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="data/1279.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a> 
              <br>
              <p> We propose a channel-wise interaction based binary convolutional neural network learning method (CI-BCNN) for efficient inference with minimal information loss. </p>
            </td>
          </tr>

      <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DCBD-MQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Deep Binary Descriptor with Multi-Quantization</papertitle>
              <br>
              Yueqi Duan, Jiwen Lu, <strong>Ziwei Wang</strong>, Jianjiang Feng, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 17.86)</em>, 2019.
              <br>
              <a href="data/multi-quantization.pdf">[PDF]</a>
              <br>
              <p> We present a deep multi-quantization network to learn a data-dependent binarization for unsupervised features. </p>
            </td>
          </tr>
    
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/COIN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
              <br>
              Yansong Tang, Dajun Ding, <strong>Yongming Rao</strong>, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a>  <a href="https://github.com/coin-dataset/annotation-tool">[Annotation Tool]</a> 
              <br>
              <p> COIN is the largest and most comprehensive instructional video analysis dataset with rich annotations. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Network Routing for Efficient Image Classification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  Jiwen Lu, Ji Lin, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 17.73)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/pami18.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a>
              <br>
              <p> We propose a generic Runtime Network Routing (RNR) framework for efficient image classification, which selects an optimal path inside the network. Our method can be applied to off-the-shelf neural network structures and
                easily extended to various application scenarios. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%"  src='images/IJCV_DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-based Face Recognition and Person Re-identification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  Jiwen Lu, Jie Zhou
              <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>, IF: 6.07)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/IJCV_DAN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video-based face recognition and person re-identification, which aims to integrate
                information from video frames for feature representation effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GODet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Globally Optimized Object Detector via Policy Gradient</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Dahua Lin, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2657.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/cvpr18_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a simple yet effective method to learn globally optimized detector for object detection by directly optimizing mAP using the REINFORCE algorithm. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Neural Pruning</papertitle>
              <br>
              Ji Lin*, <strong>Yongming Rao*</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2017
              <br>
              <a href="https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a>
              <br>
              <p> We propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-Based Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Ji Lin, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2017
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Learning_Discriminative_Aggregation_ICCV_2017_paper.pdf">[PDF]</a>  <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a> <a href="https://raoyongming.github.io/files/dan_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video face recognition, which aims to integrate information from video frames effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ADRL.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Attention-aware Deep Reinforcement Learning for Video Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Attention-Aware_Deep_Reinforcement_ICCV_2017_paper.pdf">[PDF]</a>
              <br>
              <p> We propose an attention-aware deep reinforcement learning (ADRL) method for video face recognition,
                which aims to discard the misleading and confounding frames and find the focuses of attentions in face videos for person recognition. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewer Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2018/2019/2020, ICML 2019/2020, ICCV 2019, NeurIPS 2019/2020, ICLR 2021, ECCV 2020, AAAI 2020/2021, WACV 2020/2021, ACCV 2018/2020, ICME 2019/2020, ICPR 2018/2020, ICIP 2018/2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  IJCV, T-IP, T-MM, T-Cybernetics, IEEE Access
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
