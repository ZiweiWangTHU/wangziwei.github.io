<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Ziwei Wang</title>
  
  <meta name="author" content="Ziwei Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziwei Wang</name>
              </p>
              <p>Ziwei Wang is a PhD candidate at the <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://www.au.tsinghua.edu.cn/info/1078/1627.htm">Jiwen Lu</a>.
                 He received the BS degree from the Department of Physics, Tsinghua University, China, in 2018.
                 His research interests include tiny machine learning, robotic vision and scene understanding. He has published over 10 scientific papers in the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Robotics and Automation Letters, CVPR, ICCV, ECCV and IROS. 
                 He serves as a regular reviewer member for the IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, Pattern Recognition Letters, CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, WACV, ACCV, ICPR, ICME and ICIP.
              </p>
              <p style="text-align:center">
                <a href="wang-zw18@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cMTW09EAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                 <a href="https://github.com/ZiweiWangTHU">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/icon.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/icon.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <li style="margin: 5px;" >
                <b>2022-10:</b> One paper on robotic packing is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">RAL</a>.
                <li style="margin: 5px;" >
                <b>2022-07:</b> One paper on model explanation is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
                <li style="margin: 5px;" >
                <b>2022-06:</b> One paper on robotic grasping and one paper on robotic exploration are accepted to <a href="https://iros2022.org/">IROS 2022</a>.
                <li style="margin: 5px;" >
                <b>2022-03:</b> One paper on binary representation learning is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
                <li style="margin: 5px;" >
                <b>2022-03:</b> One paper on network architecture search is accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.
                <li style="margin: 5px;" >
                <b>2021-07:</b> One paper on mixed-precision quantization and one paper on unsupervised learning are accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
               <li style="margin: 5px;" >
                <b>2021-01:</b> One paper on efficient detection is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
               <li style="margin: 5px;" >
                <b>2020-07:</b> One paper on active hashing is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-04:</b> One paper on network quantization is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
              </li>
              <li style="margin: 5px;">
                <b>2020-02:</b> One paper on efficient detection is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-02:</b> One paper on network quantization is accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
              </p>
            </td>
          </tr>
        </tbody></table>
       

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PackingPlanning.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Planning Irregular Object Packing via Hierarchical Reinforcement Learning</papertitle>
              <br>
              Sichao Huang, <strong>Ziwei Wang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Robotics and Automation Letters (<strong>RAL</strong>)</em>, 2022
              <br>
              <a href="data/PackingPlanning.pdf">[PDF]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_robot.mp4">[Robot Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_simulation.mp4">[Simulation Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing">[Code]</a>
              <br>
              <p></p>
              <p>we develop a packing planning method for general objects including the packing sequence, locations and orientations to maximize the space utilization ratio.</p>
            </td>
          </tr>
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_Shap-CAM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Shap-CAM: Visual Explanations for Convolutional Neural Networks based on Shapley Value</papertitle>
              <br>
              Quan Zheng, <strong>Ziwei Wang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>17th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
              <br>
              <a href="data/Shap-CAM.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>we develop a post-hoc visual explanation method based on the Shapley value in class activation mapping.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_Smart Explorer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Smart Explorer: Recognizing Objects in Dense Clutter via Interactive Exploration</papertitle>
              <br>
              Zhenyu Wu*, <strong>Ziwei Wang*</strong>, Zibu Wei, Yi Wei, Haibin Yan
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/Smart Explorer.pdf">[PDF]</a>
              <a href="data/IROS22_0255_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/Gary3410/Smart-Explorer">[Code]</a>
              <br>
              <p></p>
              <p>We propose an interactive exploration framework called Smart Explorer for recognizing all objects in dense clutters.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_GE-Grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GE-Grasp: Efficeint Target Oriented Grasping in Dense Clutter</papertitle>
              <br>
              Zhan Liu, <strong>Ziwei Wang</strong>, Sichao Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/GE-Grasp.pdf">[PDF]</a>
              <a href="data/IROS22_0216_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/CaptainWuDaoKou/GE-Grasp">[Code]</a>
              <br>
              <p></p>
              <p>we present a generic framework for robotic motion planning in dense clutter with diverse action primitives and generator-evaluator architectures.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/D-GraphBit.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Deep Binary Descriptors via Bitwise Interaction Mining</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Han Xiao, Yueqi Duan, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2022.
              <br>
              <a href="data/D-GraphBit.pdf">[PDF]</a> </a><a href="https://github.com/ZiweiWangTHU/GraphBit">[Code]</a>
              <br>
              <p> We propose the unsupervised binary descriptor learning method via dynamic bitwise interaction mining (D-GraphBit), where a graph convolutional network called GraphMiner reasons the optimal bitwise interaction for each input sample. </p>
            </td>
          </tr>
          
          
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Shapley-NAS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search</papertitle>
              <br>
              Han Xiao, <strong>Ziwei Wang</strong>, Zheng Zhu, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
              <br>
              <a href="data/04143.pdf">[PDF]</a>
              <a href="data/04143-supp.pdf">[Supplement]</a>
              <a href="https://github.com/Euphoria16/Shapley-NAS">[Code]</a>
              <br>
              <p></p>
              <p>We propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neural architecture search.</p>
            </td>
          </tr>
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GMPQ.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Generalizable Mixed-Precision Quantization via Attribution Rank Preservation</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Han Xiao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021.
              <br>
              <a href="data/02669.pdf">[PDF]</a>
              <a href="data/02669-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/GMPQ">[Code]</a>
              <br>
              <p></p>
              <p>We propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ISL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Instance Similarity Learning for Unsupervised Feature Representation</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Yunsong Wang, Ziyi Wu, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021.
              <br>
              <a href="data/08812.pdf">[PDF]</a>
              <a href="data/08812-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/ISL">[Code]</a>
              <br>
              <p></p>
              <p>We propose an instance similarity learning (ISL) method for unsupervised feature representation.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/AutoBiDet_Pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Efficient Binarized Object Detectors with Information Compression</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Ziyi Wu, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021, accepted.
              <br>
              <a href="data/AutoBiDet.pdf">[PDF]</a>
              <a href="data/AutoBiDet-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              <p>We propose apresent binary neural networks with automatic information compression (AutoBiDet) to automatically adjust the IB trade-off according to the input complexity.</p>
            </td>
          </tr>
          
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/HCI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-Wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>,  Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021.
              <br>
              <a href="data/channel-wise interactions.pdf">[PDF]</a> <a href="data/channel-wise interactions-supp.pdf">[Supplement]</a><a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a>
              <br>
              <p> We present a hierarchical channel-wise interaction based binary convolutional neural networks (HCI-BCNN) method to minimize the quantiztaion error for activations via hierarchical reinforcement learning. </p>
            </td>
          </tr>
          
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DH-APS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Hashing with Active Pairwise Supervision</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Quan Zheng, Jiwen Lu, Jie Zhou
              <br>
              <em>16th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="data/3281.pdf">[PDF]</a>
              <a href="data/3281-supp.pdf">[Supplement]</a>
              <a href="data/3281-slides.pdf">[Slides]</a>
              <a href="data/3281-short video.pdf">[Video]</a>
              <br>
              <p></p>
              <p>We propose a Deep Hashing method with Active Pairwise Supervision (DH-APS) to learn effective binary codes for image search with limited annotation budget.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/BiDet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BiDet: An Efficient Binarized Object Detector</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Ziyi Wu, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="data/05105.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              <p>We propose an efficient binarized object detector that fully utilizes the representational capacity of the binary neural networks by redundancy removal</strong>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Chenxin Tao, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="data/1279.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a> 
              <br>
              <p> We propose a channel-wise interaction based binary convolutional neural network learning method (CI-BCNN) for efficient inference with minimal information loss. </p>
            </td>
          </tr>

      <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DCBD-MQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Deep Binary Descriptor with Multi-Quantization</papertitle>
              <br>
              Yueqi Duan, Jiwen Lu, <strong>Ziwei Wang</strong>, Jianjiang Feng, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2019.
              <br>
              <a href="data/multi-quantization.pdf">[PDF]</a>
              <br>
              <p> We present a deep multi-quantization network to learn a data-dependent binarization for unsupervised features. </p>
            </td>
          </tr>
   

         <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GraphBit.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</papertitle>
              <br>
              Yueqi Duan, <strong>Ziwei Wang</strong>, Jiwen Lu, Xudong Lin, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
              <br>          
              <br>
              <a href="data/348.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/GraphBit">[Code]</a>
              <br>
              <p> We propose a GraphBit method to learn deep binary descriptors with enhanced reliability in a directed acyclic graph unsupervisedly. </p>
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DBD-MQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Deep Binary Descriptor with Multi-Quantization</papertitle>
              <br>
              Yueqi Duan, Jiwen Lu, <strong>Ziwei Wang</strong>, Jianjiang Feng, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2017
              <br>          
              <br>
              <a href="data/CVPR17.pdf">[PDF]</a>
              <br>
              <p> We present an unsupervised feature learning method called deep binary descriptor with multiquantization (DBD-MQ) for visual matching. </p>
            </td>
          </tr>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2020 National Scholarship</li>
                <li style="margin: 5px;"> 2018 Chi-Sun Yeh Scholarship </li>
                <li style="margin: 5px;"> 2016 Qualcomm Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2020/2021/2022, ICCV 2021, ECCV 2022, NeurIPS 2020/2021, ICML2021/2022, ICLR 2021/2022, IJCAI 2022, WACV 2020/2021/2022, ACCV 2020, ICME 2019/2020/2021/2022, ICPR 2018/2020, ICIP 2018/2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  T-IP, T-CSVT, T-BIOM, Pattern Recognition Letters, JVIC
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
