<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Ziwei Wang</title>
  
  <meta name="author" content="Ziwei Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziwei Wang</name>
              </p>
              <p>Ziwei Wang is currently a postdoc fellow in Robotics Institute, Carnegie Mellon University, supervised by Prof. Changliu Liu. He received the Ph.D and the B.S degrees from the Department of Automation, Tsinghua University in 2023 and the Department of Physics, Tsinghua University in 2018 respectively.
                 His research interests include tiny machine learning and embodied visual perception. He has published over 20 scientific papers in the IEEE Transactions on Pattern Analysis and Machine Intelligence (6 papers in TPAMI), International Journal of Computer Vision, IEEE Robotics and Automation Letters, CVPR, ICCV, ECCV, NeurIPS, IROS and ICRA. 
                 He serves as a regular reviewer member for the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Robotics and Automation Letters, Pattern Recognition, CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR and ICRA.
              </p>
              <p style="text-align:center">
                <a href="ziweiwa2@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cMTW09EAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                 <a href="https://github.com/ZiweiWangTHU">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/icon.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/icon.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <li style="margin: 5px;" >
                <b>2024-03:</b> Our paper on general robotic manipulation with dynamic Gaussian Splatting is pre-printed on Arxiv. <a href="https://arxiv.org/abs/2403.08321">[PDF]</a><a href="https://guanxinglu.github.io/ManiGaussian/">[Website]</a><a href="https://github.com/GuanxingLu/ManiGaussian">[Code]</a>
                <li style="margin: 5px;" >
                <b>2024-02:</b> Three papers are accepted to <a href="https://cvpr.thecvf.com">CVPR 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-02:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                <li style="margin: 5px;" >
                <b>2023-10:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
                <li style="margin: 5px;" >
                <b>2023-09:</b> One paper is accepted to <a href="https://nips.cc">NeurIPS 2023</a>.
                <li style="margin: 5px;" >
                <b>2023-07:</b> Our paper on embodied task planning with large language models is pre-printed on Arxiv. <a href="https://arxiv.org/abs/2307.01848">[PDF]</a><a href="https://gary3410.github.io/TaPA/">[Website]</a><a href="https://github.com/Gary3410/TaPA">[Code]</a><a href="https://huggingface.co/spaces/xuxw98/TAPA">[Demo]</a><a href="https://huggingface.co/Gary3410/pretrain_lit_llama">[Model]</a>
                <li style="margin: 5px;" >
                <b>2023-03:</b> One paper is accepted to <a href="https://www.springer.com/journal/11263">IJCV</a>.
                <li style="margin: 5px;" >
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprint</heading>
            <p>
                <li style="margin: 5px;" >
                Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang. ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation. <a href="https://arxiv.org/abs/2403.08321">[PDF]</a><a href="https://guanxinglu.github.io/ManiGaussian/">[Website]</a><a href="https://github.com/GuanxingLu/ManiGaussian">[Code]</a>
                <li style="margin: 5px;" >
                Zhenyu Wu, <strong>Ziwei Wang</strong>, Xiuwei Xu, Jie Zhou, Jiwen Lu. Embodied Task Planning with Large Language Models. <a href="https://arxiv.org/abs/2307.01848">[PDF]</a><a href="https://gary3410.github.io/TaPA/">[Website]</a><a href="https://github.com/Gary3410/TaPA">[Code]</a><a href="https://huggingface.co/spaces/xuxw98/TAPA">[Demo]</a><a href="https://huggingface.co/Gary3410/pretrain_lit_llama">[Model]</a>
            </p>
            </td>
          </tr>
        </tbody></table>
       

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
          <br>
              * means equal contribution
          <br>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/APQ-DM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Towards Accurate Data-free Quantization for Diffusion Models</papertitle>
              <br>
              Changyuan Wang, <strong>Ziwei Wang</strong>, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024.
              <br>
              [(coming soon)]</a>
              <br>
              <p> we propose a post-training quantization framework to compress diffusion models.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MCUFormer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</papertitle>
              <br>
              Yinan Liang, <strong>Ziwei Wang</strong>, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu
              <br>
              <em>Thirty-seventh Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023.
              <br>
              <a href="data/MCUFormer.pdf">[PDF]</a>
              <a href="data/MCUFormer_supp.pdf">[Supp]</a>
              <a href="https://github.com/liangyn22/MCUFormer">[Code]</a>
              <br>
              <p> we propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SeerNet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Accurate Performance Predictors for Ultrafast Automated Model Compression</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Han Xiao, Shengyu Liu, Jie Zhou
              <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2023.
              <br>
              <a href="data/SeerNet.pdf">[PDF]</a>
              <a href="https://github.com/ZiweiWangTHU/SeerNet">[Code]</a>
              <br>
              <p> We propose an ultrafast auto- mated model compression framework for flexible network deployment, where we can obtain the optimal compression policy within several seconds.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICRA23.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Category-level Shape Estimation for Densely Cluttered Objects</papertitle>
              <br>
              Zhenyu Wu, <strong>Ziwei Wang</strong>, Jiwen Lu, Haibin Yan
              <br>
              <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023.
              <br>
              <a href="data/ICRA2023.pdf">[PDF]</a>
              <a href="https://github.com/Gary3410/Shape-Estimation">[Code]</a>
              <br>
              <p> We propose a category-level shape estimation method for densely cluttered objects, which addresses the challenges of large object segmentation errors and inaccurate shape recovery on unseen instances.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Quantformer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantformer: Learning Extremely Low-precision Vision Transformers</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Changyuan Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>)</em>, 2022.
              <br>
              <a href="data/Quantformer.pdf">[PDF]</a>
              <a href="data/Quantformer-supp.pdf">[Supp]</a>
              <a href="https://github.com/ZiweiWangTHU/Quantformer">[Code]</a>
              <br>
              <p> We propose the extremely low-precision vision transformers in 2-4 bits, where the self-attention rank consistency and group-wise quantization are presented for quantization error minimization. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PackingPlanning.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Planning Irregular Object Packing via Hierarchical Reinforcement Learning</papertitle>
              <br>
              Sichao Huang, <strong>Ziwei Wang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Robotics and Automation Letters (<strong>RAL</strong>)</em>, 2022
              <br>
              <a href="data/PackingPlanning.pdf">[PDF]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_robot.mp4">[Robot Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_simulation.mp4">[Simulation Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing">[Code]</a>
              <br>
              <p></p>
              <p>we develop a packing planning method for general objects including the packing sequence, locations and orientations to maximize the space utilization ratio.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_Smart Explorer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Smart Explorer: Recognizing Objects in Dense Clutter via Interactive Exploration</papertitle>
              <br>
              Zhenyu Wu*, <strong>Ziwei Wang*</strong>, Zibu Wei, Yi Wei, Haibin Yan
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/Smart Explorer.pdf">[PDF]</a>
              <a href="data/IROS22_0255_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/Gary3410/Smart-Explorer">[Code]</a>
              <br>
              <p></p>
              <p>We propose an interactive exploration framework called Smart Explorer for recognizing all objects in dense clutters.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_GE-Grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GE-Grasp: Efficeint Target Oriented Grasping in Dense Clutter</papertitle>
              <br>
              Zhan Liu, <strong>Ziwei Wang</strong>, Sichao Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/GE-Grasp.pdf">[PDF]</a>
              <a href="data/IROS22_0216_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/CaptainWuDaoKou/GE-Grasp">[Code]</a>
              <br>
              <p></p>
              <p>we present a generic framework for robotic motion planning in dense clutter with diverse action primitives and generator-evaluator architectures.</p>
            </td>
          </tr>     
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Shapley-NAS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search</papertitle>
              <br>
              Han Xiao, <strong>Ziwei Wang</strong>, Zheng Zhu, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
              <br>
              <a href="data/04143.pdf">[PDF]</a>
              <a href="data/04143-supp.pdf">[Supplement]</a>
              <a href="https://github.com/Euphoria16/Shapley-NAS">[Code]</a>
              <br>
              <p></p>
              <p>We propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neural architecture search.</p>
            </td>
          </tr>
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GMPQ.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Generalizable Mixed-Precision Quantization via Attribution Rank Preservation</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Han Xiao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021.
              <br>
              <a href="data/02669.pdf">[PDF]</a>
              <a href="data/02669-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/GMPQ">[Code]</a>
              <br>
              <p></p>
              <p>We propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/AutoBiDet_Pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Efficient Binarized Object Detectors with Information Compression</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Ziyi Wu, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>)</em>, 2022.
              <br>
              <a href="data/AutoBiDet.pdf">[PDF]</a>
              <a href="data/AutoBiDet-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              <p>We propose apresent binary neural networks with automatic information compression (AutoBiDet) to automatically adjust the IB trade-off according to the input complexity.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/BiDet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BiDet: An Efficient Binarized Object Detector</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Ziyi Wu, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="data/05105.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              <p>We propose an efficient binarized object detector that fully utilizes the representational capacity of the binary neural networks by redundancy removal</strong>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Chenxin Tao, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="data/1279.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a> 
              <br>
              <p> We propose a channel-wise interaction based binary convolutional neural network learning method (CI-BCNN) for efficient inference with minimal information loss. </p>
            </td>
          </tr>
   
         <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GraphBit.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</papertitle>
              <br>
              Yueqi Duan, <strong>Ziwei Wang</strong>, Jiwen Lu, Xudong Lin, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
              <br>          
              <br>
              <a href="data/348.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/GraphBit">[Code]</a>
              <br>
              <p> We propose a GraphBit method to learn deep binary descriptors with enhanced reliability in a directed acyclic graph unsupervisedly. </p>
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DBD-MQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Deep Binary Descriptor with Multi-Quantization</papertitle>
              <br>
              Yueqi Duan, Jiwen Lu, <strong>Ziwei Wang</strong>, Jianjiang Feng, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2017
              <br>          
              <br>
              <a href="data/CVPR17.pdf">[PDF]</a>
              <br>
              <p> We present an unsupervised feature learning method called deep binary descriptor with multiquantization (DBD-MQ) for visual matching. </p>
            </td>
          </tr>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2023 Outstanding Doctoral Dissertation of Tsinghua University</li>
                <li style="margin: 5px;"> 2022 National Scholarship</li>
                <li style="margin: 5px;"> 2020 National Scholarship</li>
                <li style="margin: 5px;"> 2018 Chi-Sun Yeh Scholarship </li>
                <li style="margin: 5px;"> 2016 Qualcomm Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2020/2021/2022/2023, ICCV 2021/2023, ECCV 2022, NeurIPS 2020/2021/2022/2023, ICML2021/2022/2023, ICLR 2021/2022/2023, IJCAI 2022, ICRA 2023
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  T-IP, T-CSVT, R-AL, T-BIOM, Pattern Recognition, JVIC
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
